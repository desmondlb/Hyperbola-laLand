"""NLP_Final_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AQ7ja6lvaCwGZanvWPECqFTwwGMNscRr

# MOVER Paper Implementation
"""

import pandas as pd
import nltk
import numpy as np
import torch
import transformers
from transformers import BartTokenizer
from transformers import AdamW
from bart import TrainBart
from bert import TrainBert
from distilroberta import TrainDistilRoberta
import csv
import logging
from collections import defaultdict
from config import hyperparameters, PATH_HYPO_L, PATH_HYPO_XL, PATH_GLOVE_EMBED, PATH_POS_PATTERNS, MAX_INPUT_LENGTH, MAX_TARGET_LENGTH
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')


class Mover():
    def __init__(self) -> None:
        logging.basicConfig(format='%(asctime)s  %(message)s',
                            datefmt='%I:%M:%S', level=logging.INFO)

        self.hypo_xl = None
        self.hypo_l = None
        self.glove_embeddings = dict()
        self.tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')
        self.bert_tokenizer = transformers.BertTokenizer.from_pretrained(
            "bert-base-uncased")
        self.dataset = None
        # Set the model to train mode
        self.device = torch.device(
            "cuda" if torch.cuda.is_available() else "cpu")

        self.load_data()
        self.load_dataset()
        self.load_glove_embeddings()

    def load_data(self):
        """# Loading data"""
        logging.info("Reading HYPO-L and HYPO-XL")

        self.hypo_l = pd.read_csv(PATH_HYPO_L)
        f = open(PATH_HYPO_XL, 'r')
        hyperboles = f.readlines()
        self.hypo_xl = pd.DataFrame(
            hyperboles, columns=["hyperbole_sentences"])

        """# Masking"""

    def generate_pos_ngrams(self):
        # Generating POS n-grams
        logging.info("Generating POS n-grams")
        sentences = self.hypo_xl['hyperbole_sentences']

        # POS tags
        list_sentences = []
        for sentence in sentences:
            pos_tags = nltk.pos_tag(nltk.word_tokenize(sentence))
            pos_bigrams = list(nltk.bigrams(pos_tags))
            list_sentences.append(pos_bigrams)

        # For POS n-grams we will group n consecutive POS tags together

    def load_glove_embeddings(self) -> None:
        # Set the path to the GloVe embeddings file
        logging.info("Loading GloVe embeddings")

        # Load the GloVe embeddings
        with open(PATH_GLOVE_EMBED, 'r', encoding='utf-8') as f:
            for line in f:
                values = line.split()
                if len(values) != 301:
                    continue
                word = values[0]
                vector = np.asarray(values[1:], dtype='float32')
                self.glove_embeddings[word] = vector

    def get_hyperbolic_patterns(self):
        logging.info("Getting hyperbolic patterns")
        # Hyperbolic patterns
        hyperbolic_patterns = defaultdict(list)
        with open(PATH_POS_PATTERNS) as file:
            tsv_file = csv.reader(file, delimiter="\t")
            for line in tsv_file:
                llst_pos = line[0].split("+")
                hyperbolic_patterns[len(llst_pos)].append(llst_pos)
        return dict(sorted(hyperbolic_patterns.items(), key=lambda x: x[0]))

        # Given patterns match against each sentence and generate patterns

    def load_dataset(self):
        x = []
        with open("data/dataset.txt", "r") as file:
            dataset = file.readlines()
            x = eval(dataset[0])
        self.dataset = x

    def train_hyperbole_generator(self):
        trainer = TrainBart(
            optimizer=AdamW, tokenizer=self.tokenizer,
            dataset=self.dataset, glove_embed=self.glove_embeddings, device=self.device)
        # trainer.train()
        trainer.load_prev_checkpoint()
        pos_n_gram_patterns = self.get_hyperbolic_patterns()
        print(trainer.over_generate(
            "This is a well written sentence", pos_n_gram_patterns))

        generated_hyperboles = trainer.over_generate(
            "This is a well written sentence", pos_n_gram_patterns)

        bert_trainer = TrainBert(
            device=self.device, tokenizer=self.bert_tokenizer)
        bert_model = bert_trainer.finetune(5)

        distil_roberta_trainer = TrainDistilRoberta()
        distill_roberta_model = distil_roberta_trainer.finetune

        for i in generated_hyperboles:
            inputs = self.bert_tokenizer.encode_plus(
                i,
                None,
                pad_to_max_length=True,
                add_special_tokens=True,
                return_attention_mask=True,
                max_length=512,
            )
            ids = torch.tensor(inputs["input_ids"]).unsqueeze(0)
            token_type_ids = torch.tensor(
                inputs["token_type_ids"]).unsqueeze(0)
            mask = torch.tensor(inputs["attention_mask"]).unsqueeze(0)
            with torch.no_grad():
                output = bert_model(ids=ids, mask=mask,
                                    token_type_ids=token_type_ids)
                prob = torch.sigmoid(output).item()
                print(i, prob)

        sentences1 = ["Mind you don't tread in that puddle.",
                      "Mind you don't tread in that puddle.", "Mind you don't tread in that puddle."]
        sentences2 = ["Make sure you don't tread in that puddle.",
                      "Mind you don't tread in that direction.", "Mind you don't want to drown in that puddle."]

        for i in range(len(sentences1)):
            embeddings = distill_roberta_model.encode(
                [sentences1[i], sentences2[i]])
            print(sentences2[i], np.dot(embeddings[0], embeddings[1]) /
                  (np.linalg.norm(embeddings[0])*np.linalg.norm(embeddings[1])))


if __name__ == "__main__":
    ob = Mover()
    ob.train_hyperbole_generator()
