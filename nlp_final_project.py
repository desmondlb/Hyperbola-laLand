# -*- coding: utf-8 -*-
"""NLP_Final_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AQ7ja6lvaCwGZanvWPECqFTwwGMNscRr

# MOVER Paper Implementation
"""

from data_processing import DataProcessing
from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation
from torch.utils.data import Dataset
import pandas as pd
import nltk
import numpy as np
import torch
import datasets
import transformers
from torchmetrics.functional import pairwise_cosine_similarity
from transformers import AutoTokenizer, BartForConditionalGeneration, BartTokenizer
import torch.nn as nn
import torch.optim as optim
from transformers import AdamW, set_seed
from tqdm.notebook import tqdm
from nltk.util import ngrams
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader
from bart import TrainBart
import csv
import json
import logging
from collections import defaultdict
from config import hyperparameters, PATH_HYPO_L, PATH_HYPO_XL, PATH_GLOVE_EMBED, PATH_POS_PATTERNS, MAX_INPUT_LENGTH, MAX_TARGET_LENGTH
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')


class Mover():
    def __init__(self) -> None:
        logging.basicConfig(format='%(asctime)s  %(message)s',
                            datefmt='%I:%M:%S', level=logging.INFO)

        self.hypo_xl = None
        self.hypo_l = None
        self.glove_embeddings = dict()
        self.tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')
        self.dataset = None
        # Set the model to train mode
        self.device = torch.device(
            "cuda" if torch.cuda.is_available() else "cpu")

        self.load_data()
        self.load_dataset()
        self.load_glove_embeddings()

    def load_data(self):
        """# Loading data"""
        logging.info("Reading HYPO-L and HYPO-XL")

        self.hypo_l = pd.read_csv(PATH_HYPO_L)
        f = open(PATH_HYPO_XL, 'r')
        hyperboles = f.readlines()
        self.hypo_xl = pd.DataFrame(
            hyperboles, columns=["hyperbole_sentences"])

        """# Masking"""

    def generate_pos_ngrams(self):
        # Generating POS n-grams
        logging.info("Generating POS n-grams")
        sentences = self.hypo_xl['hyperbole_sentences']

        # POS tags
        list_sentences = []
        for sentence in sentences:
            pos_tags = nltk.pos_tag(nltk.word_tokenize(sentence))
            pos_bigrams = list(nltk.bigrams(pos_tags))
            list_sentences.append(pos_bigrams)

        # For POS n-grams we will group n consecutive POS tags together

    def load_glove_embeddings(self) -> None:
        # Set the path to the GloVe embeddings file
        logging.info("Loading GloVe embeddings")

        # Load the GloVe embeddings
        with open(PATH_GLOVE_EMBED, 'r', encoding='utf-8') as f:
            for line in f:
                values = line.split()
                if len(values) != 301:
                    continue
                word = values[0]
                vector = np.asarray(values[1:], dtype='float32')
                self.glove_embeddings[word] = vector

    def get_hyperbolic_patterns(self):
        logging.info("Getting hyperbolic patterns")
        # Hyperbolic patterns
        hyperbolic_patterns = defaultdict(list)
        with open(PATH_POS_PATTERNS) as file:
            tsv_file = csv.reader(file, delimiter="\t")
            for line in tsv_file:
                llst_pos = line[0].split("+")
                hyperbolic_patterns[len(llst_pos)].append(llst_pos)
        return dict(sorted(hyperbolic_patterns.items(), key=lambda x: x[0]))

        # Given patterns match against each sentence and generate patterns
    

    def load_dataset(self):
        x = []
        with open("data/dataset.txt", "r") as file:
            dataset = file.readlines()
            x = eval(dataset[0])
        self.dataset = x

    def train_hyperbole_generator(self):
        trainer = TrainBart(
            optimizer=AdamW, tokenizer=self.tokenizer, 
            dataset=self.dataset, glove_embed=self.glove_embeddings, device=self.device)
        # trainer.train()
        trainer.load_prev_checkpoint()
        pos_n_gram_patterns = self.get_hyperbolic_patterns()
        print(trainer.over_generate(
            "This is a well written sentence", pos_n_gram_patterns))


if __name__ == "__main__":
    ob = Mover()
    ob.train_hyperbole_generator()
